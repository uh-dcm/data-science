{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic models and LDA\n",
    "\n",
    "## Dataset for the exercise\n",
    "\n",
    "* [New York Times Comments](https://www.kaggle.com/aashita/nyt-comments/data), set of reders' comments for articles published in the New York Times\n",
    "\n",
    "## Overarching research question\n",
    "\n",
    "The comments allow a perspective to study what kind of concerns people raise when commenting to online articles.\n",
    "Examine if meaninful themes emerge from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data collection from files.\n",
    "## to keep the dataset fairly small, we conduct random data selection here.\n",
    "## this is *ONLY* to ensure that the model is suitable for teaching purposes\n",
    "\n",
    "path <- 'data/nyt-comments/'\n",
    "files <- list.files( path ) ## see all files in directory\n",
    "files <- files[ grepl(\"Comments\", files) ]\n",
    "files <- paste( path, files, sep = '')\n",
    "\n",
    "set.seed(1)\n",
    "\n",
    "data <- data.frame()\n",
    "\n",
    "for( file in files ){\n",
    "    d <- read.csv( file )\n",
    "    \n",
    "    data <- rbind( data, d ) ## TODO: This is a slow and poor method of doing this merging.\n",
    "}\n",
    "\n",
    "documents <- data[ runif( nrow(data) ) > .99, ]\n",
    "            \n",
    "print(\"Data sample size\" )\n",
    "print( nrow(documents) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents$commentBody <- as.character( documents$commentBody )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From text data to document-term matrix\n",
    "\n",
    "To analyse textual data we transform it to a document term matrix, where in rows we have documents (different comments) and columns represent each word in the dataset.\n",
    "\n",
    "Note how we **preprocess** the text during this quantification. We remove stopwords (through a set of common English stopwords; we could also create our own lists), stem the content of comments to ensure language is treated well and lower case everything in the content. Thus, the `document_terms` is a huge sparse matrix in the end. Preprocessing is its own kind of art, as it can [influence results](https://www.cambridge.org/core/product/identifier/S1047198717000444/type/journal_article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(quanteda)\n",
    "\n",
    "corp <- corpus( documents, text_field = \"commentBody\" )\n",
    "\n",
    "token <- tokens( corp )\n",
    "token <- tokens_select( token, pattern = stopwords('en'), selection = 'remove')\n",
    "token <- tokens_wordstem( token )\n",
    "\n",
    "document_terms <- dfm( token )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From document-term matrix to analysis\n",
    "\n",
    "Finally we run the Latent Dirichlet Allocation process to our matrix to create topics.\n",
    "Similar to k-means, we choose the number of topics; there are also other parameters which could be used to _fine tune_ topic models, see [documentation](https://www.rdocumentation.org/packages/topicmodels/) for details.\n",
    "When I use these we with their default parameters as none of them solves the challenge that [topic models work on a different abstration level than humans](http://doi.wiley.com/10.1002/asi.23786)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(topicmodels)\n",
    "\n",
    "document_terms <- convert( document_terms, to = \"topicmodels\")\n",
    "model <- LDA( document_terms, k = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms( model, 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see the distribution of a document to different topics\n",
    "posterior( model )$topics[1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* Compute the distribution of all documents to each topic. Where could you use this?\n",
    "* Modify the code and examine a few potential topic numbers. What differences can you detect?\n",
    "* Modify the preprocessing and remove all words which are shorter than four characters. What do you learn now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "There are many different approaches to evaluate topic models (see, [1](http://doi.acm.org/10.1145/1553374.1553515), [2](https://journal.fi/politiikka/article/view/79629) for examples).\n",
    "We can evaluate the suitability of topic models using statistical measurements like loglikelihood, but [some say](http://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf) that this might be a bad practice - and [others](https://journal.fi/politiikka/article/view/79629) recommend it.\n",
    "Here we show how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLik( model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* Evaluate a set of different topics based on this score. Which one would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
